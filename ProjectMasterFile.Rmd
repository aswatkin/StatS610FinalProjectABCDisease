---
title: "Approximate Bayesian Computation for Disease Outbreak"
author:
- Nasheed Jafri
- Abigail Watkins
output:
  html_document: default
  pdf_document: 
    latex_engine: xelatex
fontsize: 11pt
---

## Introduction

In this project, we apply ABC to fit parameters in a model for influenza A and B strains, based on data from past outbreaks in Michigan and Seattle. The process involves drawing parameters from prior uniform distribution, simulating data based on those parameters, computing a similarity measure between simulated and observed data, and iteratively refining the parameter estimates. 

**Objective**: The objective of this project is to replicate the model selection results presented in Section 3.3 of Tony and Stumpf's paper, "[Simulation-based model selection for dynamical systems in systems and population biology](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho)" and demonstrate how ABC can be used to estimate model parameters for complex epidemic models. In particular, we recreate Figures 3(a) and 3(c) using a 4 parameter model on two sets of observed data - Table 2 (Addy et al., 1991) and Table 3 (Longini and Koopman, 1982) from the [Supplementary Data](https://academic.oup.com/bioinformatics/article/26/1/104/182571).

## ABC using the 4-parameter model

Let $q_c$ denote the probability that a susceptible individual does not get infected from the community and $q_h$ the probability that a susceptible individual escapes infection within their household. The probability $w_{js}$, that $j$ out of the $s$ susceptible individuals in a household become infected, is then given by

$$w_{js} = \binom{s}{j}w_{jj}(q_cq_h^j)^{s-j}\, \, , s = 1,2,\dots \, \, , j = 0, 1, \dots , s$$
where $w_{0s} = q_c^s$ for $s = 0,1,2,\dots$ and $w_{ss} = 1 - \sum_{j = 0}^{s-1}w_{js}$.

We aim to infer the pair of parameters, $q_h$ and $q_c$, from the model using data from Supplementary Tables 2 and 3. In this project, we focus on the 4-parameter model with parameters $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$, which represents the hypothesis that each outbreak has its own infection-transmission rates.

### Prior distribution

Prior distributions of all parameters are chosen to be uniform over the range [0,1].

**Prior distribution:** $q_{c1}, q_{h1}, q_{c2}, q_{h2} \sim \text{Uniform}[0,1]$

In the following code, we pick 4 independent parameters from the distribution Uniform[0,1]. This returns a vector of length 4, which we use to later extract parameters $q_{c1}, q_{h1}, q_{c2}, q_{h2}$.

```{r}
prior_distribution <- function() runif(n = 4, min = 0, max = 1)    
```

### Probability distribution $w_{js}$ 

We define a function that takes parameters $q_c$, $q_h$ and (maximum) household size, and returns the probability distribution matrix `w_js_matrix` with entries $w_{js}$ for $j = 0,1,\dots, s$, $s = 1 ,2, \dots,$household_size. 

```{r}
# Function to compute w_js matrix for a given outbreak
W_js_matrix <- function(q_c, q_h, household_size) {
  
  # Throw error if q_c or q_h is not a probability
  if ((q_c < 0) || (q_h < 0) || (q_c > 1) || (q_h > 1)) {
    stop(paste("Invalid inputs:", "q_c =", q_c, "b =", q_h, "; Probabilities must be between 0 and 1."))
  }
  
  nCol <- household_size      # Columns (susceptible individuals) indexed by s = 1,..., household_size
  nRow <- household_size + 1  # Rows (infected individuals) indexed by j = 0, 1,..., s
  
  # Initialize a (zero) matrix to store the probabilities w_js
  w_js_matrix <- matrix(0, nrow = nRow , ncol = nCol)
  
  for (s in 1:nCol) {  # Iterate over number of susceptible individuals in a household
    for (j in 0:s) {   # Iterate over number of infected individuals
                       # j corresponds to row (j + 1) of the w_js matrix
      if (j == 0) {    
          w_js_matrix[j + 1,s] <- q_c^s   
      } else if (j < s) {                 
          w_js_matrix[j + 1, s] <- choose(s, j) * w_js_matrix[j + 1, j] * (q_c * q_h^j)^(s - j)
      } else {                           
          w_js_matrix[j + 1, s] <- 1 - sum(w_js_matrix[1:j,s])
      }
    }
  }
  
  rownames(w_js_matrix) <- paste("j =", 0:household_size)
  colnames(w_js_matrix) <- paste("s =", 1:household_size)
  
  return(w_js_matrix)
}
```

### Simulating data from the distribution $w_{js}$

In this section, we simulate data based on the probability distribution $w_{js}$. The function below takes as inputs the parameters $q_c$, $q_h$, (maximum) household size, and the number of households (for each household size, as a vector), and returns a matrix of simulated data. The simulation is carried out by sampling the number of infected individuals for each household size and store their counts in a matrix.

```{r}
simulate_household_data <- function(q_c, q_h, household_size, n_households) {
  
  # Throw error if user doesn't provide number of households for each household size
  if (length(n_households) != household_size) {
    stop("Invalid inputs: length(n_households) does not match (maximum) household_size.")
  }
  
  w_js_matrix <- W_js_matrix(q_c, q_h, household_size)
  
  nCol <- household_size      # Columns (susceptible individuals) indexed by s = 1,..., household_size
  nRow <- household_size + 1  # Rows (infected individuals) indexed by j = 0, 1,..., s
  
  # Initialize a (zero) matrix to store the simulated data
  simulated_data <- matrix(0, nrow = nRow, ncol = nCol)
  
  for (s in 1:nCol) {                           
                                          
    probabilities <- w_js_matrix[1:(s + 1), s]    # (j+1) corresponds to row j of w_js_matrix
    
    # Simulate household infections using random sampling
    samples <- sample(0:s, size = n_households[s], replace = TRUE, prob = probabilities)
    
    # Count the occurrences of each infection level
    counts <- table(factor(samples, levels = 0:s))
    
    # Store the simulated counts in the matrix
    simulated_data[1:(s + 1), s] <- as.numeric(counts)
  }
  
  rownames(simulated_data) <- paste("(Infected) j =", 0:household_size)
  colnames(simulated_data) <- paste("s =", 1:household_size)
  
  return(simulated_data)
}
```

We use the `sample` function to generate the samples, but the `rmultinom` function can also be used for the same purpose. 

**Example:** The following is an example of one simulated dataset, given parameters $q_c = 0.8$, $q_h = 0.6$, maximum household size of 5 and number of households same as in Table 2:

```{r}
q_c <- 0.8                                # Probability of avoiding community infection
q_h <- 0.6                                # Probability of escaping household infection
household_size <- 5                       # Maximum number of susceptible individuals in a household
n_households <- c(79, 105, 48, 44, 11)    # Number of households (same as observed data in Table 2)

simulated_data <- simulate_household_data(q_c, q_h, household_size, n_households)
print(simulated_data)
```
### Similarity measure between the simulated and observed data

To apply ABC, we use the following distance function for observed and simulated datasets.

$$d(D_{obs}, D^*) = \cfrac{1}{2}\left(\|D_1−D^∗(q_{c1},q_{h1}) \|_F + \| D_2−D^∗(q_{c2},q_{h2}) \|_F \right) \, ,$$
where 

- $D_{obs} = D_1 \cup D_2$ with $D_1$ the 1977–1978 outbreak and $D_2$ the 1980–1981 outbreak datasets from Supplementary Table 2
- $D^*$ is the simulated data
- $\| \, \|_F$ denotes the Frobenius norm of a matrix defined by $\|A\|_F = \sqrt{\sum_{i,j}|a_{ij}|^2}$.

We define the said distance function that takes two pairs of observed and simulated data and returns the average Frobenius distance between them. 

```{r}
frobenius_norm <- function(A) return(sqrt(sum(A^2)))

distance <- function(given_data1, given_data2, simulated_data1, simulated_data2){
  # Throw error if given_data1 and simulated_data1 aren't the same size
  if(ncol(given_data1)!=ncol(simulated_data1) || nrow(given_data1) != nrow(simulated_data1)){
    stop("Invalid inputs: given_data1 and simulated_data1 have different dimensions")
  }
  
  # Throw error if given_data2 and simulated_data2 aren't the same size
  if(ncol(given_data2)!=ncol(simulated_data2) || nrow(given_data2) != nrow(simulated_data2)){
    stop("Invalid inputs: given_data1 and simulated_data1 have different dimensions")
  }
  
  distance1 = frobenius_norm(given_data1-simulated_data1)
  distance2 = frobenius_norm(given_data2-simulated_data2)
  total_distance = (distance1+distance2)/2
  return(total_distance)
}
```

### Generating ABC samples

The ABC algorithm for this project is given below.

**Inputs:**

* 4-parameter vector: $q = (q_{c1}, q_{h1}, q_{c2}, q_{h2})$

* Target posterior: $P(q|D_{obs}) \propto P(D_{obs} | q)P(q)$

* A way of simulating from $P(D_{obs}|q) \sim w_{js}$

* Prior on the parameters: $P(q) \sim \text{Uniform}[0,1]$

* Similarity measure: $d(D_{obs}, D^*) = \frac{1}{2} \left( \| D_1 - D^*(q_{c1}, q_{h1}) \|_F + \| D_2 - D^*(q_{c2}, q_{h2}) \|_F \right)$

* Tolerance $\epsilon$

**Sampling:** for $i = 1, 2, \dots, N$

* Generate $q^{(i)} \sim \text{Uniform}[0,1]$

* Generate $D^{*(i)} \sim w_{js}$

**Accept/Reject Criterion:**

* If $d(D_{obs}, D^{*(i)}) < \epsilon$, accept $q^{(i)}$, else reject.

**Posterior Approximation:**

* The accepted parameter values approximate the posterior distribution $P(q|D_{obs})$.

#### Function to Generate Posterior Samples

Next, we generate posterior samples by defining a function that takes the pair of observed data (given_data1, given_data2), a similarity measure (distance), a prior distribution for the parameter, a data-simulating function, and a tolerance level. This function returns a posterior sample of the parameter $q$ (in our case, the 4-parameter vector $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$) that satisfies the tolerance condition for the distance between simulated and given data.

```{r}
generate_abc_sample <- function(given_data1, given_data2,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    
    #Throw error if tolerance level is not positive
    if(epsilon <= 0){
      stop("Epsilon must be positive")
    }
        
    while(TRUE) {
        q <- prior_distribution()
        
        # Throw error if prior_distribution is not of the right shape
        if(as.double(length(q))!=4){
          stop("Prior_distribution must have exactly 4 entries")
        }
      
        q_c1 <- q[1]
        q_h1 <- q[2]
        q_c2 <- q[3]
        q_h2 <- q[4]
        
        household_size1 <- ncol(given_data1)
        n_households1 <- colSums(given_data1)
        
        simulated_data1 <- data_simulating_function(q_c1, q_h1, household_size1, n_households1)
        
        household_size2 <- ncol(given_data2)
        n_households2 <- colSums(given_data2)
        
        simulated_data2 <- data_simulating_function(q_c2, q_h2, household_size2, n_households2)
        
        if(distance(given_data1, given_data2, simulated_data1, simulated_data2) < epsilon) {
            return(q)
        }
    }
}
```

## Observed Data (Table 2)

The following code stores the observed data from Table 2 of the Supplementary Material. 

- given_data1: Influenza A (H3N2) infection in 1977-78, Tecumseh, Michigan. [3]
- given_data2: Influenza A (H3N2) infection in 1980-81, Tecumseh, Michigan. [3]  

```{r}
given_data1 <- matrix(c(66, 87, 25, 22, 4,
                        13, 14, 15, 9, 4,
                        0, 4, 4, 9, 1,
                        0, 0, 4, 3, 1,
                        0, 0, 0, 1, 1, 
                        0, 0, 0, 0, 0), byrow = TRUE, ncol=5)

given_data2 <- matrix(c(44, 62, 47, 38, 9,
                        10, 13, 8, 11, 5,
                        0, 9, 2, 7, 3,
                        0, 0, 3, 5, 1,
                        0, 0, 0, 1, 0, 
                        0, 0, 0, 0, 1), byrow = TRUE, ncol=5)

```

### Applying ABC to get posterior samples from Observed Data from Table 2

#### Note about tolerance level $\epsilon$ 

Before applying ABC to get sample parameters, we make a small note about what is considered a reasonable tolerance level. Our datasets represent counts of households with $s$ susceptible and $j$ infected individuals, structured as a 6×5 matrix. **By design, these counts are always integers**. Notably, 10 entries in each dataset are fixed to zero, corresponding to cases where $j>s$. The remaining 20 entries are randomly generated using the simulate_household_data function.

Since both observed data and simulated data are 6×5 matrices with integer entries, and 10 of the entries are always zero, the Frobenius distance can be sensitive to small differences. For instance, if 5 of the 20 entries are identical and remaining 15 differ by only 1, the Frobenius distance becomes 15. This scale justifies setting $\epsilon = 15$ as a reasonable threshold, which is what we use to generate the posteriors.

Now we are ready to apply our ABC sample generating function to get posterior samples for $(q_{c1}, q_{h1}, q_{c2}, q_{h2})$.

```{r}
prior_distribution <- function() runif(n = 4, min = 0, max = 1)     
# Generating one posterior sample
generate_abc_sample(given_data1, given_data2, distance,
                        prior_distribution, simulate_household_data, epsilon = 15)
```

The code below produces 100 parameter samples that are accepted by the algorithm with $\epsilon = 15$. This is a computationally expensive process and took about 2 hours to run on RStudio.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation, eval=FALSE}
# Generating 100 posterior samples
posterior_samples <- replicate(n = 100,
    generate_abc_sample(given_data1, given_data2, distance,
                        prior_distribution, simulate_household_data, epsilon = 15))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save posterior_samples_Table2.rds on your local machine (from Drive) before running this
# saveRDS(posterior_samples, "posterior_samples_Table2.rds")
posterior_samples <- readRDS("posterior_samples_Table2.rds")
```

Here are a few posterior samples from the 100 generated above.

```{r}
posterior_samples[, 95:100]
```

We now extract individual parameters $q_{c1}$, $q_{h1}$, $q_{c2}$ and $q_{h2}$ from the samples. 

```{r}
# Extracting samples of parameters q_c1, q_h1, q_c2, q_h2 from the posterior samples
q_c1 <- posterior_samples[1, ]  # First row for q_c1
q_h1 <- posterior_samples[2, ]  # Second row for q_h1
q_c2 <- posterior_samples[3, ]  # Third row for q_c2
q_h2 <- posterior_samples[4, ]  # Fourth row for q_h2
```

Finally, we plot the simulated parameters $q_h$ vs $q_c$ for the two outbreaks. This recreates Figure 3(a) from [Toni and Stumpf (2010)](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho) using the Supplementary data in Table 2. As in the original paper, we use \textcolor{red}{red} for the 1977 outbreak ($q_{h1}, q_{c1}$) and \textcolor{blue}{blue} for the 1980 outbreak ($q_{h2}, q_{c2}$).


```{r}
# Recreate Figure 3(a) with n = 100 posterior samples and epsilon = 15

par(pty = "s")      

plot(q_h1, q_c1, 
     main = "q_h vs q_c (n = 100, epsilon = 15)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h2, q_c2) in blue
points(q_h2, q_c2, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data1 from Table 2", "given_data2 from Table 2"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```

## Observed Data (Table 3)

The following code stores the observed data from Table 3 of the Supplementary Material. 

- given_data3: Influenza B infection in 1975-76, Seattle, Washington. [4]
- given_data4: Influenza A (H1N1) infection in 1978-79, Seattle, Washington. [4]  

```{r}
given_data3 <- matrix(c(9, 12, 18, 9, 4,
                        1, 6, 6, 4, 3,
                        0, 2, 3, 4, 0,
                        0, 0, 1, 3, 2,
                        0, 0, 0, 0, 0, 
                        0, 0, 0, 0, 0), byrow = TRUE, ncol=5)

given_data4 <- matrix(c(15, 12, 4,
                        11, 17, 4,
                        0, 21, 4,
                        0, 0, 5 ), byrow = TRUE, ncol=3)

```

### Applying ABC to get posterior samples from Observed Data from Table 3

As with data from Table 2, we now apply our sample generating function to get posterior samples for $(q_{c3}, q_{h3}, q_{c4}, q_{h4})$. Since the given data in Table 3 is smaller than the one in Table 2, we are able to choose a smaller $\epsilon$ and still be within computational limits. We use $\epsilon = 7$ as a reasonable tolerance level.

```{r}
# Generating one posterior sample 

generate_abc_sample(given_data3, given_data4, distance, 
                    prior_distribution, simulate_household_data, epsilon = 7)
```

The code below produces 200 parameter samples that are accepted by the algorithm with $\epsilon = 7$.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-2, eval=FALSE}
# Generating 200 posterior samples
posterior_samples2 <- replicate(n = 200,
    generate_abc_sample(given_data3, given_data4, distance, 
                        prior_distribution, simulate_household_data, epsilon = 7))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save posterior_samples_Table3.rds on your local machine (from Drive) before running this
# saveRDS(posterior_samples2, "posterior_samples_Table3.rds")
posterior_samples2 <- readRDS("posterior_samples_Table3.rds")
```

Here are a few posterior samples from the 200 generated above.

```{r}
posterior_samples2[, 95:100]
```

We now extract individual parameters $q_{c3}$, $q_{h3}$, $q_{c4}$ and $q_{h4}$ from the samples. 

```{r}
# Extracting samples of parameters q_c3, q_h3, q_c4, q_h4 from the posterior samples
q_c3 <- posterior_samples2[1, ]  # First row for q_c3
q_h3 <- posterior_samples2[2, ]  # Second row for q_h3
q_c4 <- posterior_samples2[3, ]  # Third row for q_c4
q_h4 <- posterior_samples2[4, ]  # Fourth row for q_h4
```

Finally, we plot the simulated parameters $q_h$ vs $q_c$ for the two outbreaks. This recreates Figure 3(b) from [Toni and Stumpf (2010)](https://watermark.silverchair.com/bioinformatics_26_1_104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3swggN3BgkqhkiG9w0BBwagggNoMIIDZAIBADCCA10GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMjV15kHzFknWstbVZAgEQgIIDLvGZcFAgMxJ2FtxTGrsPvkAoO-imsFvwyY2RJRbPWpz_WYOR3ZYIIXnJpmCp_pOOlhB9fGwvPCCBNkFN7jjQvo-jtSs3vYGT9U9ABO1ngcGxGq0M_-xfz5QDNcfLJpdHphXjvXPmNQKw-FbmW7Z-lM4VADhWgRMXeAa69IcQWbf3O3M4YVlAfOhNibTRLt8QLpayutZlbZAwX6aC2a13wmjnKF6Vx3WJWazbewssqJov9CmNXprFKUqnhcq1QLZ4oaGSKYaxVFpmwB2ZylzUBbliQ3fYN6VRAfleLXrmyvOymid2GtXNnhrslyx6SN2OSgbXU0YIgfSgCk5OaCETsFY7VMGzLCuUTB776n6hDJKcZ-Hb7RelIJxLeOZteaxRVOiu-a9pG5NbQQuueQtS0C-kqHlVksEwUAucqzS9UXX3ucvmsIgYK-jQQ8jmtqPTjVkdFGhR1J3LzOw7VJCJQy4b_a_WZLDNS7bskxvvZgU7DOZAVHxYu1aPUHh3UaeJ-5oMwJ-sqFWg_6ZruUPk4L9f1KB1siRgSmxw-Eo4JHKXjSEsIXAylD3m_trgxEIxkeqgXFJ867U-qJxeG39ToS9BptAG_IGK-HfMD0ovPK9mKHXvrp32fRO5S0oiqaCMa8kV4DGwbZjaMArJDV9Ps3WNw_EE2E8m7J4UjiqLNQkihUtUM6d4xmJ-S4zo-qPJkr0ajWkDhQwkeJ1wsaYGXItivcoAB4lzyQmG3Zs5kQQIIa2m4hveEf2mDlglHMoPHTAN5hGG-9_LegexhFcKAZTguF4nNpozqAVsIQaj8DeAaHWY8AvjP5HjDTgYHs4ni3w7EjULGDSroFhBndTpCAMNjtY9yIqoh248Bf7ayWtBCXUx1yJyIamAPGeHej3nPnf80TACr2Of6fJicQ-hFcdVGzQj8qiq8b9GuOFFJ43SnZudftdAwwlA0mQb30ZhvgJvsYngGY752-NegVEd2F_r6N2Jkw-G-fzEnoObf6OXzGaYlNSq4_s4vGnQ38HGe_HPk7fdDIESUA35j1SbK5K83274IWvReND0Byubb2MmgZ4fJ90sCCKnjLeu9ho) using the Supplementary data in Table 3. As in the original paper, we use \textcolor{red}{red} for the 1975 outbreak ($q_{h3}, q_{c3}$) and \textcolor{blue}{blue} for the 1978 outbreak ($q_{h4}, q_{c4}$).


```{r}
# Recreate Figure 3(b) with n = 200 posterior samples and epsilon = 7

par(pty = "s")      

plot(q_h3, q_c3, 
     main = "q_h vs q_c (n = 200, epsilon = 7)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h4, q_c4) in blue
points(q_h4, q_c4, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data3 from Table 3", "given_data4 from Table 3"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```


## Two Parameter Model

The plot comparing models of outbreaks of the same strain of the virus suggests that different outbreaks of the same strain of a virus can be modeled using the same parameters. To this end, in this section we write functions for the two parameter model described by Toni and Stumpf and use this model to find parameters for the model of influenza A. This process follows the process for the four parameter model closely. We can reuse the functions W_js_matrix and simulate_household_data without any changes. Below we rework the functions prior_distribution, distance, and generate_abc_samples to work with the two parameter model.

```{r}
# Returns 2 random values from the uniform prior distribution on [0,1]

prior_distribution_2 <- function() runif(n = 2, min = 0, max = 1)  

```

```{r}
# Uses the Frobenius norm to return the distance between two data sets "given_data" and "simulated_data"

distance_2 <- function(given_data, simulated_data){
  # Throw error if given_data and simulated_data aren't the same size
  if(ncol(given_data)!=ncol(simulated_data) || nrow(given_data) != nrow(simulated_data)){
    stop("Invalid inputs: given_data and simulated_data have different dimensions")
  }

  total_distance = frobenius_norm(given_data-simulated_data)
  return(total_distance)
}

```

```{r}
# This function generates posteriors for the 2 parameter model

generate_abc_sample_2 <- function(given_data,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    
    #Throw error if tolerance level is not positive
    if(epsilon <= 0){
      stop("Epsilon must be positive")
    }
        
    while(TRUE) {
        q <- prior_distribution()
        
        # Throw error if prior_distribution is not of the right shape
        if(as.double(length(q))!=2){
          stop("Prior_distribution must have exactly 2 entries")
        }
      
        q_c <- q[1]
        q_h <- q[2]
        
        household_size <- ncol(given_data)
        n_households <- colSums(given_data)
        
        simulated_data <- data_simulating_function(q_c, q_h, household_size, n_households)
        
        if(distance_2(given_data, simulated_data) < epsilon) {
            return(q)
        }
    }
}
```

### 2 Parameter Model for Influenza A Outbreaks

Now with these modified functions we are able to build a 2 parameter model for influenza A. To do this we will first combine the data given for the 1977-78 and 1980-81 outbreaks of influenza A into one matrix by adding the matrices for the separate outbreaks.

```{r}
combined_influenza_data <- given_data1 + given_data2

```

Using this data set, we can use the 2 parameter functions from above to generate many posterior samples. Here we use the same value for $\epsilon$ as in the 4 parameter model.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-3, eval=FALSE}
combined_influenza_posterior_samples <- replicate(n = 100, generate_abc_sample_2(combined_influenza_data, distance_2,
                        prior_distribution_2, simulate_household_data, epsilon = 15))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save combined_influenza_posterior_samples.rds on your local machine before running this
# saveRDS(combined_influenza_posterior_samples, "combined_influenza_posterior_samples.rds")
combined_influenza_posterior_samples <- readRDS("combined_influenza_posterior_samples.rds")
```

Here are a few posterior samples from the 100 generated above.

```{r}
combined_influenza_posterior_samples[, 95:100]

```

We can represent all of this data in a scatterplot similar to the ones above.

```{r}
par(pty = "s")      

plot(combined_influenza_posterior_samples[2,], combined_influenza_posterior_samples[1,], 
     main = "q_h vs q_c (n = 100, epsilon = 15)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "blue", 
     cex = 1.5)

```


One way to choose parameters to model the spread of influenza A would be to take $q_c$ and $q_h$ to be the average of the parameters stored in generated by this 2 parameter ABC approach.

```{r}
influenza_a_q_c <- mean(combined_influenza_posterior_samples[1,])

influenza_a_q_h <- mean(combined_influenza_posterior_samples[2,])

print(paste("Mean q_c is", round(influenza_a_q_c,4) , "and mean q_h is" , round(influenza_a_q_h,4)))
  
```


```{r}
median(combined_influenza_posterior_samples[1,])
```

## Three Parameter Model

The plot comparing models of the outbreaks of different strains of the virus suggests that outbreaks of different strains have the same $q_h$ but different $q_c$. This can be modeled using a 3-parameter model same parameters $(q_{c1},q_{c2},q_h)$. We can again reuse the functions `W_js_matrix`, `simulate_household_data` and `distance` without any changes. Below we rework the functions `prior_distribution`, and `generate_abc_samples` to work with the three parameter model.


```{r}
# Returns 3 random values from the uniform prior distribution on [0,1]

prior_distribution_3 <- function() runif(n = 3, min = 0, max = 1)  

```


```{r}
# This function generates posteriors for the 3 parameter model
generate_abc_sample3 <- function(given_data1, given_data2,
                                distance,
                                prior_distribution,
                                data_simulating_function,
                                epsilon) {
    
    #Throw error if tolerance level is not positive
    if(epsilon <= 0){
      stop("Epsilon must be positive")
    }
        
    while(TRUE) {
        q <- prior_distribution()
        
        # Throw error if prior_distribution is not of the right shape
        if(as.double(length(q))!=3){
          stop("Prior_distribution must have exactly 3 entries")
        }
      
        q_c1 <- q[1]
        q_c2 <- q[2]
        q_h <- q[3]
        
        household_size1 <- ncol(given_data1)
        n_households1 <- colSums(given_data1)
        
        simulated_data1 <- data_simulating_function(q_c1, q_h, household_size1, n_households1)
        
        household_size2 <- ncol(given_data2)
        n_households2 <- colSums(given_data2)
        
        simulated_data2 <- data_simulating_function(q_c2, q_h, household_size2, n_households2)
        
        if(distance(given_data3, given_data4, simulated_data1, simulated_data2) < epsilon) {
            return(q)
        }
    }
}
```

### 3 Parameter Model for Influenza A and Influenz B Outbreaks

Now with these modified functions we are able to build a 3 parameter model for influenza A and influenza B. We use the same data again as given in Table 3.

```{r}
given_data3 <- matrix(c(9, 12, 18, 9, 4,
                        1, 6, 6, 4, 3,
                        0, 2, 3, 4, 0,
                        0, 0, 1, 3, 2,
                        0, 0, 0, 0, 0, 
                        0, 0, 0, 0, 0), byrow = TRUE, ncol=5)

given_data4 <- matrix(c(15, 12, 4,
                        11, 17, 4,
                        0, 21, 4,
                        0, 0, 5 ), byrow = TRUE, ncol=3)

```

Using this data set, we can use the 3 parameter functions from above to generate many posterior samples. Here we again use $\epsilon = 7$ for this smaller dataset and generate 200 samples.

```{r, include=FALSE, echo = FALSE, results='hide'}
# DO NOT RUN THE CELL BELOW THIS ONE!
# It will take forever. Have already run it and saved the output as RDS to access it again.
# Otherwise it creates problems in knitting and tries to draw samples again.  
```

```{r long-computation-4, eval=FALSE}
# Generating 200 posterior samples
influenzaAB_samples <- replicate(n = 200,
    generate_abc_sample3(given_data3, given_data4, distance, 
                        prior_distribution_3, simulate_household_data, epsilon = 7))
```


```{r, include=FALSE, echo = FALSE, results='hide'}
# Save influenzaAB_samples.rds on your local machine before running this
# saveRDS(influenzaAB_samples, "influenzaAB_samples.rds")
influenzaAB_samples <- readRDS("influenzaAB_samples.rds")
```

Here are a few posterior samples from the 200 generated above.

```{r}
influenzaAB_samples[, 195:200]
```
We now extract individual parameters $q_{c1}$, $q_{c2}$ and $q_{h}$ from the samples. 

```{r}
# Extracting samples of parameters q_c3, q_h3, q_c4, q_h4 from the posterior samples
q_c_1 <- influenzaAB_samples[1, ]  # First row for q_c_1
q_c_2 <- influenzaAB_samples[2, ]  # Second row for q_c_2
q_h <- influenzaAB_samples[3, ]  # Third row for q_h
```

We can represent all of this data in a scatterplot similar to the ones above.

```{r}
par(pty = "s")      

# Add first set of data (q_h, q_c_1) in red
plot(q_h, q_c_1, 
     main = "q_h vs q_c (n = 200, epsilon = 7)", 
     xlab = "q_h", ylab = "q_c", 
     xlim = c(0, 1), ylim = c(0, 1), 
     pch = 16, col = "red", 
     cex = 1.5)  

# Add second set of data (q_h, q_c_2) in blue
points(q_h, q_c_2, pch = 16, col = "blue", cex = 1.2)

# Add a legend
legend("bottomleft", 
       legend = c("given_data3 from Table 3", "given_data4 from Table 3"), 
       col = c("red", "blue"), 
       pch = 16, bty = "n")
```

## Conclusion 
xxxxxxxxxx

## Appendix : Function Tests

### Tests for W_js_matrix

```{r}
# Tests for W_js_matrix

library(testthat)

test_that("W_js_matrix throws error for bad parameters", {
  expect_error(W_js_matrix(0.5, -0.1, 4))        # Negative q_h
  expect_error(W_js_matrix(2, 0.9, 3))           # q_c > 1
  expect_error(W_js_matrix(0.3, 0.2, -7))        # household_size not a positive integer
})

test_that("W_js_matrix works for simple cases", {
  # q_h = q_c = 0 
  W0 <- W_js_matrix(0,0,3)
  expected_W0 <- matrix(c(0,0,0,
                         1,0,0,
                         0,1,0,
                         0,0,1), ncol = 3, byrow = T)
  expect_equivalent(W0, expected_W0)            # expect_equal will fail because of attribute comparison 
                                                # W0 has rownames and colnames, while expected_W0 does not
  # q_h = q_c = 1 
  W1 <- W_js_matrix(1,1,4)
  expected_W1 <- matrix(c(1,1,1,1,
                         0,0,0,0,
                         0,0,0,0,
                         0,0,0,0,
                         0,0,0,0), ncol = 4, byrow = T)
  expect_equivalent(W1, expected_W1)            # W1 has rownames and colnames, while expected_W1 does not
})
  
test_that("W_js_matrix columns sum to 1", {
  for (i in 1:100) {                            # Test for 100 random cases
    q_c <- runif(1, 0, 1)                       # Choose q_c randomly between 0 and 1
    q_h <- runif(1, 0, 1)                       # Choose q_h randomly between 0 and 1
    household_size <- sample(1:10, 1)           # Choose household_size randomly between 1 and 10

    W <- W_js_matrix(q_c, q_h, household_size)
    
    expect_equivalent(colSums(W), rep(1, ncol(W)))   
  }
})
```


### Tests for simulate_household_data

```{r}
# Tests for simulate_household_data

library(testthat)

test_that("simulate_household_data throws error for bad parameters", {
  expect_error(simulate_household_data(0.5, -0.1, 3, c(12,10,6)))   # Negative q_h
  expect_error(simulate_household_data(1.4, 0.8, 4, c(12,10,6,3)))  # q_c > 1
  expect_error(simulate_household_data(0.3, 0.4, 6, c(100,90,8)))   # length(n_households) != household_size 
})

test_that("simulate_household_data works for simple cases", {
  # 0 number of households for household size 3 should return third column 0
  data.0 <- simulate_household_data(0.1, 0.8, 4, c(12,8,0,10))

  expect_equivalent(data.0[,3], rep(0,nrow(data.0)))  
})
  
test_that("simulate_household_data doesn't have more infected than susceptible", {
  q_c <- runif(1, 0, 1)                          # Choose q_c randomly between 0 and 1
  q_h <- runif(1, 0, 1)                          # Choose q_h randomly between 0 and 1
  household_size <- sample(1:10, 1)              # Random household size between 1 and 10
  n_households <- sample(1:100, household_size)  # Random n_households vector of size household_size 
  
  result <- simulate_household_data(q_c, q_h, household_size, n_households)
  
  # Validate that entries for j > s are 0
  for (j in 1 : nrow(result)) {             
    for (s in 1 : ncol(result)) {           
      if (j-1 > s) {                             # j corresponds to row (j-1)
        expect_equivalent(result[j, s], 0)    
      }
    }
  }
})
```

### Tests for Frobenius norm

```{r}
# Tests for frobenius_norm

library(testthat)

test_that("frobenius_norm works on simple cases: zero matrix",{
  # The norm of the 0 matrix should be zero
  zero_matrix <-matrix(c(0,0,0,0,0,0,0,0,0), nrow=3)
  
  expect_equivalent(frobenius_norm(zero_matrix), 0)
})

test_that("frobenius_norm works on simple cases: c(1,2,0,-2)",{
  # Checks that the Frobenius norm is 3 on this simple case
  example_matrix <-matrix(c(1,2,0,-2), nrow=2)
  
  expect_equivalent(frobenius_norm(example_matrix), 3)
})

```

### Tests for distance

```{r}
# Tests for distance

library(testthat)

test_that("distance throws error for bad parameters", {
  size1 = matrix(1, nrow=1)
  size2 = matrix(c(2,2,2,2), nrow =2)
  
  expect_error(distance(size1, size1, size2, size1))   # Given_data_1 and simulated_data_1 have different sizes
  expect_error(distance(size1, size1, size1, size2))  # Given_data_2 and simulated_data_2 have different sizes
})

test_that("distance works on simple cases: distance between given data and simulated data is 0 when given and simulated data are identical",{
  data_1 <- matrix(c(1,1,1,1), nrow=2)
  data_2 <- matrix(c(2,2,2,2), nrow=2)
  
  expect_equivalent(distance(data_1, data_2, data_1, data_2), 0)
})

test_that("distance works on simple cases",{
  given_data_1 <- matrix(c(1,1,1,1), nrow=2)
  given_data_2 <- matrix(c(2,2,2,2), nrow=2)
  simulated_data_both <- matrix(c(0,0,0,0), nrow = 2)
  
  expect_equivalent(distance(given_data_1, given_data_2, simulated_data_both,  simulated_data_both), 3)
})

```

### Tests for generate_abc_sample

```{r}
# Tests for generate_abc_sample

library(testthat)

test_that("generate_abc_sample throws errors for bad parameters", {
  
  expect_error(generate_abc_sample(given_data3, given_data4, distance, 
                    prior_distribution, simulate_household_data, epsilon = -1)) # negative epsilon
  expect_error(generate_abc_sample(given_data3, given_data4, distance, 
                    prior_distribution, simulate_household_data, epsilon = 0)) # zero epsilon
  expect_error(generate_abc_sample(given_data3, given_data4, distance, 
                    prior_distribution = c(1,2,3), simulate_household_data, epsilon = 20)) # wrong size prior_distribution
})

```





## Bibliography

[1] Toni T, Stumpf M.P.H. Simulation-based model selection for dynamical systems in systems and population biology. Bioinformatics, 104–110, 2010.

[2] Toni T, Stumpf M.P.H. Supplementary figures and datasets to [1]. 

[3] Addy C, Jr IL and Haber M. A generalized stochastic model for the analysis of infectious disease final size data. Biometrics, 961–974, 1991.

[4] Jr IL and Koopman J. Household and community transmission parameters from final distribu- tions of infections in households. Biometrics, 115–126, 1982.